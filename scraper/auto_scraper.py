#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
QueenAlba ìë™ ìŠ¤í¬ë˜í¼ v1.0
ëª©ì : ê´‘ê³  ëª©ë¡ì—ì„œ ìƒì„¸ í˜ì´ì§€ê¹Œì§€ ìë™ ìˆ˜ì§‘
í•µì‹¬: undetected_chromedriverë¡œ ë´‡ ê°ì§€ ìš°íšŒ

ì‚¬ìš©ë²•:
    python auto_scraper.py                    # ê¸°ë³¸ ì‹¤í–‰
    python auto_scraper.py --max 10           # ìµœëŒ€ 10ê°œë§Œ ìˆ˜ì§‘
    python auto_scraper.py --headless         # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
"""

import sys
import io
import os
import json
import time
import random
import re
import base64
from datetime import datetime
from urllib.parse import urljoin, urlparse

# Windows ì½˜ì†” UTF-8 ì„¤ì •
try:
    if sys.stdout.encoding != 'utf-8':
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
except Exception:
    pass

try:
    import undetected_chromedriver as uc
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
except ImportError:
    print("âŒ í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   pip install undetected-chromedriver selenium")
    sys.exit(1)

# ===== ì„¤ì • =====
BASE_URL = "https://queenalba.net"
CREDENTIALS_FILE = "credentials.py"
COOKIES_FILE = "cookies_queenalba.json"

# ê²½ë¡œ ì •ì±…ì— ë”°ë¥¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
try:
    from utils.paths import PATHS, get_dated_filename
    OUTPUT_FILE = str(PATHS["reports_daily"] / "scraped_ads.json")
    COLLECTED_URLS_FILE = str(PATHS["base"] / "collected_urls.json")
except ImportError:
    # í´ë°±: ê¸°ì¡´ ê²½ë¡œ ì‚¬ìš©
    OUTPUT_FILE = "../src/data/scraped_ads.json"
    COLLECTED_URLS_FILE = "collected_urls.json"


def load_credentials():
    """ì €ì¥ëœ ê³„ì • ì •ë³´ ë¡œë“œ"""
    try:
        if os.path.exists(CREDENTIALS_FILE):
            with open(CREDENTIALS_FILE, 'r', encoding='utf-8') as f:
                content = f.read()
                exec_globals = {}
                exec(content, exec_globals)
                creds = exec_globals.get('QUEENALBA_CREDENTIALS', {})
                return creds.get('username'), creds.get('password')
    except Exception as e:
        print(f"âš ï¸ ê³„ì • ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return None, None


def save_cookies(driver, site_url):
    """ì¿ í‚¤ ì €ì¥"""
    try:
        cookies = driver.get_cookies()
        with open(COOKIES_FILE, 'w', encoding='utf-8') as f:
            json.dump(cookies, f, ensure_ascii=False, indent=2)
        print(f"ğŸª ì¿ í‚¤ ì €ì¥ë¨")
    except Exception as e:
        print(f"âš ï¸ ì¿ í‚¤ ì €ì¥ ì‹¤íŒ¨: {e}")


def load_cookies(driver):
    """ì¿ í‚¤ ë¡œë“œ"""
    try:
        if os.path.exists(COOKIES_FILE):
            with open(COOKIES_FILE, 'r', encoding='utf-8') as f:
                cookies = json.load(f)
            for cookie in cookies:
                try:
                    driver.add_cookie(cookie)
                except:
                    pass
            print(f"ğŸª ì¿ í‚¤ ë¡œë“œë¨")
            return True
    except Exception as e:
        print(f"âš ï¸ ì¿ í‚¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return False


def load_collected_urls():
    """ì´ë¯¸ ìˆ˜ì§‘í•œ URL ë¡œë“œ"""
    try:
        if os.path.exists(COLLECTED_URLS_FILE):
            with open(COLLECTED_URLS_FILE, 'r', encoding='utf-8') as f:
                return set(json.load(f))
    except:
        pass
    return set()


def save_collected_urls(urls):
    """ìˆ˜ì§‘í•œ URL ì €ì¥"""
    try:
        with open(COLLECTED_URLS_FILE, 'w', encoding='utf-8') as f:
            json.dump(list(urls), f, ensure_ascii=False, indent=2)
    except:
        pass


class QueenAlbaAutoScraper:
    def __init__(self, headless=False, max_ads=None, delay=2):
        self.headless = headless
        self.max_ads = max_ads
        self.delay = delay
        self.results = []
        self.collected_urls = load_collected_urls()
        self.driver = None
        
        print("=" * 60)
        print("ğŸš€ QueenAlba ìë™ ìŠ¤í¬ë˜í¼ v1.0")
        print("=" * 60)
        print(f"ğŸ“Œ Headless: {headless}")
        print(f"ğŸ“Œ ìµœëŒ€ ìˆ˜ì§‘: {max_ads if max_ads else 'ë¬´ì œí•œ'}")
        print(f"ğŸ“Œ ë”œë ˆì´: {delay}ì´ˆ")
        print(f"ğŸ“Œ ê¸°ì¡´ ìˆ˜ì§‘: {len(self.collected_urls)}ê°œ URL")
        print()
        
        self._init_driver()
    
    def _init_driver(self):
        """undetected-chromedriver ì´ˆê¸°í™” (ë´‡ ê°ì§€ ìš°íšŒ)"""
        print("ğŸ”§ Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")
        
        options = uc.ChromeOptions()
        
        # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™” (ì†ë„ í–¥ìƒ)
        prefs = {"profile.managed_default_content_settings.images": 2}
        options.add_experimental_option("prefs", prefs)
        
        if self.headless:
            options.add_argument('--headless=new')
        
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--window-size=1920,1080')
        
        try:
            self.driver = uc.Chrome(
                options=options,
                version_main=142,  # Chrome ë²„ì „ 142ì— ë§ì¶¤
                use_subprocess=True
            )
            self.wait = WebDriverWait(self.driver, 20)
            print("âœ… ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ (Bot Detection Bypass)\n")
        except Exception as e:
            print(f"âŒ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _wait_and_find(self, by, value, timeout=10):
        """ìš”ì†Œê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°"""
        try:
            return WebDriverWait(self.driver, timeout).until(
                EC.presence_of_element_located((by, value))
            )
        except TimeoutException:
            return None
    
    def _check_login_status(self):
        """ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸"""
        try:
            indicators = [
                "//a[contains(text(), 'ë¡œê·¸ì•„ì›ƒ')]",
                "//a[contains(@href, 'logout')]",
                "//a[contains(text(), 'ë§ˆì´í˜ì´ì§€')]",
            ]
            for xpath in indicators:
                if self.driver.find_elements(By.XPATH, xpath):
                    return True
            return False
        except:
            return False
    
    def _map_field_value(self, data, label, value, extracted):
        """í•„ë“œ ë§¤í•‘ í—¬í¼ (fallbackìš©)"""
        import re
        label = label.replace(' ', '')
        
        if ('ë‹‰ë„¤ì„' in label or 'ë‹´ë‹¹ì' in label) and 'nickname' not in extracted:
            data['advertiser']['nickname'] = value.split('\n')[0].strip()
            extracted['nickname'] = True
        elif ('ì „í™”' in label or 'ì—°ë½ì²˜' in label) and 'phone' not in extracted:
            phone_match = re.search(r'0\d{1,2}-?\d{3,4}-?\d{4}', value)
            data['advertiser']['phone'] = phone_match.group(0) if phone_match else value.split()[0] if value.split() else ''
            extracted['phone'] = True
        elif ('ì¹´í†¡' in label or 'ì¹´ì¹´ì˜¤' in label) and 'kakao' not in extracted:
            data['advertiser']['kakao_id'] = value.split()[0] if value.split() else value
            extracted['kakao'] = True
        elif 'ìƒí˜¸' in label and 'íšŒì‚¬' not in label and 'business' not in extracted:
            data['advertiser']['business_name'] = value.split('|')[0].strip()
            extracted['business'] = True
        elif ('ê·¼ë¬´ì§€ì—­' in label or label == 'ì§€ì—­') and 'location' not in extracted:
            data['advertiser']['work_location'] = value.split('\n')[0].strip()
            extracted['location'] = True
        elif ('ì—…ë¬´' in label or 'ì—…ì¢…' in label or 'ì§ì¢…' in label) and 'job_type' not in extracted:
            data['recruitment']['job_type'] = value.split('\n')[0].strip()
            extracted['job_type'] = True
        elif 'ê¸‰ì—¬' in label and 'salary' not in extracted:
            salary_match = re.search(r'([\d,]+ì›)', value)
            data['recruitment']['salary'] = salary_match.group(1) if salary_match else value.split()[0] if value.split() else ''
            extracted['salary'] = True
    
    def login(self):
        """ë¡œê·¸ì¸ ì²˜ë¦¬"""
        print("=" * 60)
        print("ğŸ” ë¡œê·¸ì¸ ë° ì„±ì¸ ì¸ì¦")
        print("=" * 60)
        
        try:
            print(f"ğŸ“¡ {BASE_URL} ì ‘ì† ì¤‘...")
            self.driver.get(BASE_URL)
            self._wait_and_find(By.TAG_NAME, "body")
            time.sleep(2)
            
            # 1. ì¿ í‚¤ë¡œ ë¡œê·¸ì¸/ì„±ì¸ì¸ì¦ ì‹œë„
            if load_cookies(self.driver):
                print("ğŸª ì¿ í‚¤ ì ìš© í›„ ìƒˆë¡œê³ ì¹¨...")
                self.driver.refresh()
                self._wait_and_find(By.TAG_NAME, "body")
                time.sleep(2)
            
            # 2. ì„±ì¸ ì¸ì¦ í˜ì´ì§€ ì²´í¬ (adult_index.php) - ë¡œê·¸ì¸ í•„ìš”
            current_url = self.driver.current_url
            if 'adult_index' in current_url or 'adult' in current_url:
                print("ğŸ” ì„±ì¸ ì¸ì¦ í˜ì´ì§€ ê°ì§€! (ë¡œê·¸ì¸ í•„ìš”)")
                
                # ê³„ì • ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                username, password = load_credentials()
                if username and password:
                    print(f"ğŸ”‘ ì„±ì¸ ì¸ì¦ í˜ì´ì§€ì—ì„œ ë¡œê·¸ì¸: {username}")
                    
                    # ì•„ì´ë”” ì…ë ¥
                    id_field = self._wait_and_find(By.XPATH, "//input[@name='mb_id' or @name='user_id' or @type='text']")
                    if id_field:
                        id_field.clear()
                        id_field.send_keys(username)
                        print("  âœ“ ì•„ì´ë”” ì…ë ¥ ì™„ë£Œ")
                    
                    # ë¹„ë°€ë²ˆí˜¸ ì…ë ¥
                    pw_field = self._wait_and_find(By.XPATH, "//input[@type='password']")
                    if pw_field:
                        pw_field.clear()
                        pw_field.send_keys(password)
                        print("  âœ“ ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ ì™„ë£Œ")
                    
                    # ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
                    login_btn = self._wait_and_find(By.XPATH, "//input[@type='submit' or @type='image'] | //button[contains(text(), 'ë¡œê·¸ì¸')]")
                    if login_btn:
                        login_btn.click()
                        print("  âœ“ ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­")
                    
                    time.sleep(3)
                
                # ì„±ì¸ ì¸ì¦ ë²„íŠ¼ í´ë¦­ ì‹œë„ (ë¡œê·¸ì¸ í›„)
                adult_buttons = [
                    "//a[contains(text(), 'ì„±ì¸ì…ë‹ˆë‹¤') or contains(text(), 'ì˜ˆ')]",
                    "//button[contains(text(), 'ì„±ì¸ì…ë‹ˆë‹¤') or contains(text(), 'ì˜ˆ')]",
                    "//input[contains(@value, 'ì„±ì¸') or contains(@value, 'ì˜ˆ')]",
                    "//a[contains(@href, 'adult') and contains(text(), 'ì…ì¥')]",
                    "//a[contains(text(), '19ì„¸')]",
                    "//img[contains(@alt, 'ì„±ì¸')]/.."  # ì´ë¯¸ì§€ë¥¼ ê°ì‹¸ëŠ” a íƒœê·¸
                ]
                
                for selector in adult_buttons:
                    try:
                        btn = self._wait_and_find(By.XPATH, selector, timeout=2)
                        if btn:
                            print(f"  âœ“ ì„±ì¸ ì¸ì¦ ë²„íŠ¼ ë°œê²¬, í´ë¦­...")
                            btn.click()
                            time.sleep(3)
                            break
                    except:
                        continue
                
                # ë©”ì¸ í˜ì´ì§€ ì´ë™ ë° í™•ì¸
                self.driver.get(BASE_URL)
                time.sleep(2)
                
                # ì—¬ì „íˆ ì„±ì¸ ì¸ì¦ í˜ì´ì§€ë¼ë©´ ìˆ˜ë™ ëŒ€ê¸°
                if 'adult_index' in self.driver.current_url:
                    print("âš ï¸ ì„±ì¸ ì¸ì¦ ìš°íšŒ ì‹¤íŒ¨. ë¸Œë¼ìš°ì €ì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ì¸ì¦í•´ì£¼ì„¸ìš”...")
                    print("   ì¸ì¦ í›„ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”.")
                    input()
                
                # ì¿ í‚¤ ì €ì¥
                save_cookies(self.driver, BASE_URL)
                print("ğŸª ì„±ì¸ ì¸ì¦ ì¿ í‚¤ ì €ì¥ ì™„ë£Œ")
            
            # 3. ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
            if self._check_login_status():
                print("âœ… ë¡œê·¸ì¸ ì„±ê³µ!")
                return True
            
            # 4. ë¡œê·¸ì¸ ì‹œë„
            login_url = urljoin(BASE_URL, '/bbs/login.php')
            print(f"ğŸ“¡ ë¡œê·¸ì¸ í˜ì´ì§€: {login_url}")
            self.driver.get(login_url)
            self._wait_and_find(By.TAG_NAME, "body")
            time.sleep(1)
            
            # ê³„ì • ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            username, password = load_credentials()
            if not username or not password:
                print("âš ï¸ ê³„ì • ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. credentials.pyë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                print("ğŸ’¡ ë¸Œë¼ìš°ì €ì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ë¡œê·¸ì¸ í›„ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
                input()
            else:
                print(f"ğŸ”‘ ê³„ì • ì‚¬ìš©: {username}")
                
                # ì•„ì´ë”” ì…ë ¥
                id_field = self._wait_and_find(By.XPATH, "//input[@name='mb_id' or @type='text']")
                if id_field:
                    id_field.clear()
                    id_field.send_keys(username)
                
                # ë¹„ë°€ë²ˆí˜¸ ì…ë ¥
                pw_field = self._wait_and_find(By.XPATH, "//input[@type='password']")
                if pw_field:
                    pw_field.clear()
                    pw_field.send_keys(password)
                
                # ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
                login_btn = self._wait_and_find(By.XPATH, "//input[@type='submit' or @type='image'] | //button[contains(text(), 'ë¡œê·¸ì¸')]")
                if login_btn:
                    login_btn.click()
                
                time.sleep(3)
            
            # ë¡œê·¸ì¸ í™•ì¸ ë° ì¿ í‚¤ ì €ì¥
            if self._check_login_status():
                save_cookies(self.driver, BASE_URL)
                print("âœ… ë¡œê·¸ì¸ ì™„ë£Œ!")
                return True
            else:
                print("âš ï¸ ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ ë¶ˆê°€ (ê³„ì† ì§„í–‰)")
                return True
            
        except Exception as e:
            print(f"âŒ ë¡œê·¸ì¸ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def get_ad_list(self):
        """ë©”ì¸ í˜ì´ì§€ì—ì„œ ê´‘ê³  ëª©ë¡ ìˆ˜ì§‘"""
        print("\n" + "=" * 60)
        print("ğŸ“‹ ê´‘ê³  ëª©ë¡ ìˆ˜ì§‘")
        print("=" * 60)
        
        ad_links = []
        
        try:
            print(f"ğŸ“¡ ë©”ì¸ í˜ì´ì§€ ì ‘ì†: {BASE_URL}")
            self.driver.get(BASE_URL)
            self._wait_and_find(By.TAG_NAME, "body")
            time.sleep(3)  # í˜ì´ì§€ ì™„ì „ ë¡œë“œ ëŒ€ê¸°
            
            # í˜ì´ì§€ URL í™•ì¸ (ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²´í¬)
            current_url = self.driver.current_url
            print(f"ğŸ“ í˜„ì¬ URL: {current_url}")
            
            # í˜ì´ì§€ ì†ŒìŠ¤ ê¸¸ì´ í™•ì¸
            page_source = self.driver.page_source
            print(f"ğŸ“„ í˜ì´ì§€ ì†ŒìŠ¤ ê¸¸ì´: {len(page_source)} ë¬¸ì")
            
            # "ì˜ëª»ëœ ì ‘ê·¼" ì²´í¬
            if "ì˜ëª»ëœ ì ‘ê·¼" in page_source or "ë¡œê·¸ì¸" in self.driver.title:
                print("âš ï¸ ì˜ëª»ëœ ì ‘ê·¼ ë˜ëŠ” ë¡œê·¸ì¸ í•„ìš” ê°ì§€")
                # ì¿ í‚¤ ë‹¤ì‹œ ë¡œë“œ ì‹œë„
                if load_cookies(self.driver):
                    self.driver.refresh()
                    time.sleep(3)
            
            # guin_detail.php ë§í¬ ì°¾ê¸°
            links = self.driver.find_elements(By.XPATH, "//a[contains(@href, 'guin_detail')]")
            print(f"âœ… guin_detail ë§í¬: {len(links)}ê°œ")
            
            # ë‹¤ë¥¸ íŒ¨í„´ë„ ì‹œë„
            if len(links) == 0:
                print("ğŸ” ë‹¤ë¥¸ íŒ¨í„´ìœ¼ë¡œ ë§í¬ íƒìƒ‰...")
                # a íƒœê·¸ì—ì„œ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•œ ë§í¬ ì°¾ê¸°
                img_links = self.driver.find_elements(By.XPATH, "//a[.//img]")
                print(f"   ì´ë¯¸ì§€ í¬í•¨ ë§í¬: {len(img_links)}ê°œ")
                
                # ëª¨ë“  a íƒœê·¸ ê°œìˆ˜
                all_links = self.driver.find_elements(By.TAG_NAME, "a")
                print(f"   ì „ì²´ a íƒœê·¸: {len(all_links)}ê°œ")
                
                # í˜ì´ì§€ ì œëª© ì¶œë ¥
                print(f"   í˜ì´ì§€ ì œëª©: {self.driver.title}")
            
            seen_urls = set()
            for link in links:
                try:
                    href = link.get_attribute('href')
                    if href and href not in seen_urls:
                        # URLì—ì„œ num íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                        match = re.search(r'num=(\d+)', href)
                        if match:
                            ad_id = int(match.group(1))
                            
                            # ì´ë¯¸ ìˆ˜ì§‘ëœ URL ìŠ¤í‚µ
                            if href in self.collected_urls:
                                continue
                            
                            seen_urls.add(href)
                            ad_links.append({
                                'url': href,
                                'id': ad_id
                            })
                except:
                    continue
            
            print(f"âœ… ìƒˆë¡œìš´ ê´‘ê³ : {len(ad_links)}ê°œ (ì¤‘ë³µ ì œì™¸)")
            
            return ad_links[:self.max_ads] if self.max_ads else ad_links
            
        except Exception as e:
            print(f"âŒ ê´‘ê³  ëª©ë¡ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def scrape_detail(self, url, ad_id):
        """ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        print(f"\n{'='*60}")
        print(f"[{ad_id}] {url}")
        print(f"{'='*60}")
        
        try:
            self.driver.get(url)
            self._wait_and_find(By.TAG_NAME, "body")
            time.sleep(random.uniform(0.5, 1.5))  # ë´‡ íƒì§€ íšŒí”¼
            
            data = {
                'id': ad_id,
                'url': url,
                'scraped_at': datetime.now().isoformat(),
                'title': '',
                'advertiser': {
                    'nickname': '',
                    'call_number': '',
                    'call_mgmt_number': '',
                    'phone': '',
                    'kakao_id': '',
                    'telegram_id': '',
                    'business_name': '',
                    'work_location': '',
                    'views': 0
                },
                'recruitment': {
                    'job_type': '',
                    'employment_type': '',
                    'salary': '',
                    'deadline': '',
                    'benefits': [],
                    'keywords': []
                },
                'detail': {
                    'description': '',
                    'images': []
                },
                'company': {
                    'company_name': '',
                    'company_address': '',
                    'representative': ''
                },
                'thumbnail': '',
                'location': '',
                'pay': '',
                'phones': [],
                'content': '',
                'detail_images': []
            }
            
            # ===== 1. ê´‘ê³  ì´ë¯¸ì§€ ì¶”ì¶œ (wys2/file_attach íŒ¨í„´) =====
            images = self.driver.find_elements(By.TAG_NAME, "img")
            for img in images:
                try:
                    src = img.get_attribute('src') or ''
                    if 'wys2/file_attach' in src:
                        if not src.startswith('http'):
                            src = BASE_URL + src
                        if src not in data['detail']['images']:
                            data['detail']['images'].append(src)
                            print(f"  âœ“ ì´ë¯¸ì§€: {src.split('/')[-1]}")
                except:
                    continue
            
            print(f"  ğŸ“· ì´ {len(data['detail']['images'])}ê°œ ê´‘ê³  ì´ë¯¸ì§€")
            
            # ===== 2. í…Œì´ë¸”ì—ì„œ ì—…ì²´ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „) =====
            extracted = {}
            
            # 2-1. ì§ì ‘ XPathë¡œ ì£¼ìš” í•„ë“œ ì¶”ì¶œ (ê°€ì¥ ì •í™•í•¨)
            field_selectors = {
                'nickname': ["//td[.//b[contains(text(), 'ë‹‰ë„¤ì„')]]/following-sibling::td", "//th[contains(text(), 'ë‹‰ë„¤ì„')]/following-sibling::td"],
                'phone': ["//td[.//b[contains(text(), 'ì „í™”')]]/following-sibling::td", "//th[contains(text(), 'ì „í™”')]/following-sibling::td"],
                'kakao': ["//td[.//b[contains(text(), 'ì¹´í†¡') or contains(text(), 'ì¹´ì¹´ì˜¤')]]/following-sibling::td", "//tr[@class='kakao-wrap']//td[not(.//b)]"],
                'telegram': ["//td[.//b[contains(text(), 'í…”ë ˆê·¸ë¨')]]/following-sibling::td", "//tr[@class='telegram-wrap']//td[not(.//b)]"],
                'business': ["//td[.//b[contains(text(), 'ìƒí˜¸')]]/following-sibling::td", "//th[contains(text(), 'ìƒí˜¸')]/following-sibling::td"],
                'location': ["//td[.//b[contains(text(), 'ê·¼ë¬´ì§€ì—­') or contains(text(), 'ì§€ì—­')]]/following-sibling::td", "//th[contains(text(), 'ì§€ì—­')]/following-sibling::td"],
                'job_type': ["//td[.//b[contains(text(), 'ì—…ì¢…') or contains(text(), 'ì—…ë¬´')]]/following-sibling::td", "//th[contains(text(), 'ì—…ì¢…')]/following-sibling::td"],
                'salary': ["//td[.//b[contains(text(), 'ê¸‰ì—¬')]]/following-sibling::td", "//th[contains(text(), 'ê¸‰ì—¬')]/following-sibling::td"],
                'deadline': ["//td[.//b[contains(text(), 'ê´‘ê³ ê¸°ê°„')]]/following-sibling::td", "//th[contains(text(), 'ê´‘ê³ ê¸°ê°„')]/following-sibling::td"],
                'company_name': ["//td[.//b[contains(text(), 'íšŒì‚¬ëª…')]]/following-sibling::td", "//th[contains(text(), 'íšŒì‚¬ëª…')]/following-sibling::td"],
                'company_address': ["//td[.//b[contains(text(), 'íšŒì‚¬ì£¼ì†Œ')]]/following-sibling::td", "//th[contains(text(), 'íšŒì‚¬ì£¼ì†Œ')]/following-sibling::td"],
                'representative': ["//td[.//b[contains(text(), 'ëŒ€í‘œì')]]/following-sibling::td", "//th[contains(text(), 'ëŒ€í‘œì')]/following-sibling::td"],
            }
            
            for field, selectors in field_selectors.items():
                if field in extracted:
                    continue
                for selector in selectors:
                    try:
                        elem = self.driver.find_element(By.XPATH, selector)
                        value = elem.text.strip().split('\n')[0]
                        # JavaScript ì½”ë“œ í•„í„°ë§
                        if value and 'function' not in value and '$.ajax' not in value and len(value) < 200:
                            if field == 'nickname':
                                data['advertiser']['nickname'] = value
                                print(f"  âœ“ ë‹‰ë„¤ì„: {value}")
                            elif field == 'phone':
                                phone_match = re.search(r'0\d{1,2}-?\d{3,4}-?\d{4}', value)
                                data['advertiser']['phone'] = phone_match.group(0) if phone_match else value.split()[0] if value.split() else ''
                                if data['advertiser']['phone']:
                                    print(f"  âœ“ ì „í™”ë²ˆí˜¸: {data['advertiser']['phone']}")
                            elif field == 'kakao':
                                data['advertiser']['kakao_id'] = value.split()[0] if value.split() else value
                                print(f"  âœ“ ì¹´í†¡ID: {data['advertiser']['kakao_id']}")
                            elif field == 'telegram':
                                data['advertiser']['telegram_id'] = value.split()[0] if value.split() else value
                            elif field == 'business':
                                data['advertiser']['business_name'] = value.split('|')[0].strip()
                                print(f"  âœ“ ìƒí˜¸: {data['advertiser']['business_name']}")
                            elif field == 'location':
                                data['advertiser']['work_location'] = value
                                print(f"  âœ“ ê·¼ë¬´ì§€ì—­: {value}")
                            elif field == 'job_type':
                                data['recruitment']['job_type'] = value
                                print(f"  âœ“ ì—…ì¢…: {value}")
                            elif field == 'salary':
                                salary_match = re.search(r'([\d,]+ì›)', value)
                                data['recruitment']['salary'] = salary_match.group(1) if salary_match else value.split()[0] if value.split() else ''
                                if data['recruitment']['salary']:
                                    print(f"  âœ“ ê¸‰ì—¬: {data['recruitment']['salary']}")
                            elif field == 'deadline':
                                data['recruitment']['deadline'] = value
                            elif field == 'company_name':
                                data['company']['company_name'] = value
                                print(f"  âœ“ íšŒì‚¬ëª…: {value}")
                            elif field == 'company_address':
                                data['company']['company_address'] = value
                            elif field == 'representative':
                                data['company']['representative'] = value
                            extracted[field] = True
                            break
                    except:
                        continue
            
            # 2-2. Fallback: í…Œì´ë¸” í–‰ ìˆœíšŒ (th-td, td-td íŒ¨í„´)
            if not extracted:
                rows = self.driver.find_elements(By.XPATH, "//table//tr")
                for row in rows:
                    try:
                        ths = row.find_elements(By.TAG_NAME, 'th')
                        tds = row.find_elements(By.TAG_NAME, 'td')
                        
                        # th-td êµ¬ì¡°
                        if ths and tds:
                            label = ths[0].text.strip().split('\n')[0].replace(' ', '')
                            value = tds[0].text.strip()
                            if value and 'function' not in value and len(value) < 200:
                                self._map_field_value(data, label, value, extracted)
                        
                        # td-td êµ¬ì¡° (ì²« ë²ˆì§¸ tdì— b íƒœê·¸)
                        elif len(tds) >= 2:
                            for i in range(0, len(tds) - 1, 2):
                                try:
                                    label_td = tds[i]
                                    value_td = tds[i + 1]
                                    try:
                                        label_elem = label_td.find_element(By.TAG_NAME, 'b')
                                        label = label_elem.text.strip()
                                    except:
                                        label = label_td.text.strip()
                                    value = value_td.text.strip()
                                    if label and value and 'function' not in value and len(value) < 200:
                                        self._map_field_value(data, label, value, extracted)
                                except:
                                    continue
                    except:
                        continue
            
            # ===== 3. ì¡°íšŒìˆ˜ ì¶”ì¶œ =====
            try:
                page_text = self.driver.find_element(By.TAG_NAME, 'body').text
                views_match = re.search(r'ì¡°íšŒ[:\s]*([\d,]+)', page_text)
                if views_match:
                    data['advertiser']['views'] = int(views_match.group(1).replace(',', ''))
                    print(f"  âœ“ ì¡°íšŒìˆ˜: {data['advertiser']['views']}")
            except:
                pass
            
            # ===== 4. íƒ€ì´í‹€ ë° ì¸ë„¤ì¼ ì„¤ì • =====
            data['title'] = data['advertiser']['nickname'] or data['advertiser']['business_name'] or f"ê´‘ê³  #{ad_id}"
            
            if data['detail']['images']:
                data['thumbnail'] = data['detail']['images'][0]
            
            # Legacy fields
            data['location'] = data['advertiser']['work_location']
            data['pay'] = data['recruitment']['salary']
            data['phones'] = [data['advertiser']['phone']] if data['advertiser']['phone'] else []
            data['detail_images'] = data['detail']['images']
            
            print(f"  âœ… ì™„ë£Œ\n")
            return data
            
        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {e}\n")
            return None
    
    def scrape_all(self):
        """ì „ì²´ ìŠ¤í¬ë˜í•‘"""
        # ë¡œê·¸ì¸
        if not self.login():
            print("âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨ë¡œ ì¢…ë£Œ")
            return
        
        # ê´‘ê³  ëª©ë¡ ìˆ˜ì§‘
        ad_links = self.get_ad_list()
        if not ad_links:
            print("âŒ ìˆ˜ì§‘í•  ê´‘ê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ“Š ì´ {len(ad_links)}ê°œ ê´‘ê³  ìŠ¤í¬ë˜í•‘ ì‹œì‘...")
        
        start_time = time.time()
        
        for i, ad in enumerate(ad_links):
            # ì§„í–‰ë¥  í‘œì‹œ
            percent = int((i / len(ad_links)) * 100)
            print(f"\n[{i+1}/{len(ad_links)}] ({percent}%) ì²˜ë¦¬ ì¤‘...")
            
            # ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
            detail = self.scrape_detail(ad['url'], ad['id'])
            
            if detail:
                self.results.append(detail)
                self.collected_urls.add(ad['url'])
                
                # 5ê°œë§ˆë‹¤ ìë™ ë°±ì—…
                if len(self.results) % 5 == 0:
                    self.save_results()
                    save_collected_urls(self.collected_urls)
                    print(f"ğŸ’¾ ìë™ ë°±ì—…: {len(self.results)}ê°œ ì €ì¥")
            
            # ë”œë ˆì´ (ë´‡ íƒì§€ íšŒí”¼)
            time.sleep(random.uniform(self.delay * 0.5, self.delay * 1.5))
        
        # ìµœì¢… ì €ì¥
        self.save_results()
        save_collected_urls(self.collected_urls)
        
        elapsed = time.time() - start_time
        print("\n" + "=" * 60)
        print(f"âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
        print(f"ğŸ“Š ìˆ˜ì§‘ëœ ê´‘ê³ : {len(self.results)}ê°œ")
        print(f"â±ï¸ ì†Œìš” ì‹œê°„: {int(elapsed // 60)}ë¶„ {int(elapsed % 60)}ì´ˆ")
        print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {OUTPUT_FILE}")
        print("=" * 60)
    
    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        try:
            # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
            existing = []
            if os.path.exists(OUTPUT_FILE):
                with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                    existing = json.load(f)
            
            # ID ê¸°ì¤€ ì¤‘ë³µ ì œê±°
            existing_ids = {ad['id'] for ad in existing}
            for ad in self.results:
                if ad['id'] not in existing_ids:
                    existing.append(ad)
                    existing_ids.add(ad['id'])
            
            # ì €ì¥
            os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
            with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                json.dump(existing, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {len(existing)}ê°œ ê´‘ê³ ")
            
        except Exception as e:
            print(f"âŒ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def close(self):
        """ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("ğŸ‘‹ ë¸Œë¼ìš°ì € ì¢…ë£Œ")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='QueenAlba ìë™ ìŠ¤í¬ë˜í¼')
    parser.add_argument('--max', type=int, help='ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜')
    parser.add_argument('--headless', action='store_true', help='ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰')
    parser.add_argument('--delay', type=float, default=2, help='ë”œë ˆì´(ì´ˆ)')
    
    args = parser.parse_args()
    
    scraper = None
    try:
        scraper = QueenAlbaAutoScraper(
            headless=args.headless,
            max_ads=args.max,
            delay=args.delay
        )
        scraper.scrape_all()
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
        if scraper:
            scraper.save_results()
            save_collected_urls(scraper.collected_urls)
    finally:
        if scraper:
            scraper.close()


if __name__ == "__main__":
    main()
